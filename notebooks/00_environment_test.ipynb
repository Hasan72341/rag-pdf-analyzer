{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92261fc7",
   "metadata": {},
   "source": [
    "# GenAI Development Environment - Quick Start\n",
    "\n",
    "This notebook demonstrates the key features of your GenAI development environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32437843",
   "metadata": {},
   "source": [
    "## 1. Test Local LLM with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "\n",
    "# Test Ollama connection\n",
    "try:\n",
    "    response = requests.get('http://ollama:11434/api/tags')\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"‚úÖ Ollama is running!\")\n",
    "        print(f\"üì¶ Available models: {[model['name'] for model in models.get('models', [])]}\")\n",
    "    else:\n",
    "        print(\"‚ùå Ollama is not responding\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
    "    print(\"üí° Make sure to run './setup-models.sh' to download models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with local model (requires model to be downloaded)\n",
    "try:\n",
    "    response = ollama.chat(\n",
    "        model='qwen2.5:7b',  # Change to available model\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': 'Explain machine learning in simple terms'}\n",
    "        ]\n",
    "    )\n",
    "    print(\"ü§ñ Local LLM Response:\")\n",
    "    print(response['message']['content'])\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Download models first: './setup-models.sh'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3e77a",
   "metadata": {},
   "source": [
    "## 2. Test Vector Database (Qdrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9056c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Connect to Qdrant\n",
    "try:\n",
    "    client = QdrantClient(host=\"qdrant\", port=6333)\n",
    "    print(\"‚úÖ Connected to Qdrant\")\n",
    "    \n",
    "    # Get collections\n",
    "    collections = client.get_collections()\n",
    "    print(f\"üìÇ Collections: {collections}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Qdrant: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3252b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and store in Qdrant\n",
    "try:\n",
    "    # Load embedding model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Sample documents\n",
    "    documents = [\n",
    "        \"Machine learning is a subset of artificial intelligence\",\n",
    "        \"Neural networks are inspired by biological neurons\",\n",
    "        \"Deep learning uses multiple layers of neural networks\",\n",
    "        \"Natural language processing helps computers understand text\"\n",
    "    ]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(documents)\n",
    "    print(f\"‚úÖ Generated {len(embeddings)} embeddings of dimension {embeddings.shape[1]}\")\n",
    "    \n",
    "    # Create collection\n",
    "    collection_name = \"test_collection\"\n",
    "    \n",
    "    try:\n",
    "        client.delete_collection(collection_name)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.COSINE)\n",
    "    )\n",
    "    \n",
    "    # Insert vectors\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=[\n",
    "            {\n",
    "                \"id\": i,\n",
    "                \"vector\": embedding.tolist(),\n",
    "                \"payload\": {\"text\": doc}\n",
    "            }\n",
    "            for i, (embedding, doc) in enumerate(zip(embeddings, documents))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Stored {len(documents)} documents in Qdrant\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa765e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search similar documents\n",
    "try:\n",
    "    query = \"What is artificial intelligence?\"\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    print(f\"üîç Search results for: '{query}'\")\n",
    "    for result in results:\n",
    "        print(f\"  üìÑ Score: {result.score:.3f} - {result.payload['text']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b822f56",
   "metadata": {},
   "source": [
    "## 3. Test LangChain with Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f58140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize local LLM\n",
    "try:\n",
    "    llm = OllamaLLM(\n",
    "        model=\"qwen2.5:7b\",  # Change to your available model\n",
    "        base_url=\"http://ollama:11434\"\n",
    "    )\n",
    "    \n",
    "    # Test simple prompt\n",
    "    response = llm.invoke(\"What are the benefits of using local LLMs?\")\n",
    "    print(\"ü§ñ LangChain + Local LLM Response:\")\n",
    "    print(response)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Make sure you have downloaded a model with './setup-models.sh'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f8585",
   "metadata": {},
   "source": [
    "## 4. Test Database Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import redis\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Test PostgreSQL\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        database=\"genai_db\", \n",
    "        user=\"genai_user\",\n",
    "        password=\"genai_password\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    version = cursor.fetchone()\n",
    "    print(f\"‚úÖ PostgreSQL: {version[0][:50]}...\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PostgreSQL Error: {e}\")\n",
    "\n",
    "# Test Redis\n",
    "try:\n",
    "    r = redis.Redis(host='redis', port=6379, db=0)\n",
    "    r.set('test_key', 'GenAI Dev Environment')\n",
    "    value = r.get('test_key').decode('utf-8')\n",
    "    print(f\"‚úÖ Redis: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Redis Error: {e}\")\n",
    "\n",
    "# Test Elasticsearch\n",
    "try:\n",
    "    es = Elasticsearch([{'host': 'elasticsearch', 'port': 9200}])\n",
    "    info = es.info()\n",
    "    print(f\"‚úÖ Elasticsearch: {info['version']['number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Elasticsearch Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c462f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Qdrant Vector Database\n",
    "try:\n",
    "    client = QdrantClient(host=\"qdrant\", port=6333)\n",
    "    print(\"‚úÖ Qdrant: Connected successfully\")\n",
    "    collections = client.get_collections()\n",
    "    print(f\"üìÇ Collections: {len(collections.collections)} found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Qdrant Error: {e}\")\n",
    "\n",
    "# Test Ollama Local LLM Server\n",
    "try:\n",
    "    response = requests.get('http://ollama:11434/api/version')\n",
    "    if response.status_code == 200:\n",
    "        version_info = response.json()\n",
    "        print(f\"‚úÖ Ollama: Version {version_info.get('version', 'unknown')}\")\n",
    "        \n",
    "        # List available models\n",
    "        models_response = requests.get('http://ollama:11434/api/tags')\n",
    "        if models_response.status_code == 200:\n",
    "            models = models_response.json()\n",
    "            model_list = [model['name'] for model in models.get('models', [])]\n",
    "            print(f\"ü§ñ Available models: {model_list if model_list else 'No models downloaded yet'}\")\n",
    "        else:\n",
    "            print(\"üì¶ No models loaded yet\")\n",
    "    else:\n",
    "        print(\"‚ùå Ollama is not responding\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama Error: {e}\")\n",
    "    print(\"üí° Make sure to run './setup-models.sh' after environment starts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151522e",
   "metadata": {},
   "source": [
    "## 5. Test OpenAI API (if configured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Test OpenAI API (requires API key in .env)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key != 'your_openai_api_key_here':\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Say hello from the GenAI dev environment!\"}\n",
    "            ],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        print(\"‚úÖ OpenAI API Response:\")\n",
    "        print(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OpenAI API Error: {e}\")\n",
    "else:\n",
    "    print(\"üí° OpenAI API key not configured. Add it to .env file to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1743f",
   "metadata": {},
   "source": [
    "## üéâ Environment Test Complete!\n",
    "\n",
    "If all tests above passed, your GenAI development environment is ready!\n",
    "\n",
    "### Next Steps:\n",
    "1. Download AI models: `./setup-models.sh`\n",
    "2. Configure API keys in `.env` file\n",
    "3. Explore the example projects in `/workspace/projects/`\n",
    "4. Build your own AI applications!\n",
    "\n",
    "### Useful Resources:\n",
    "- üìä Qdrant Dashboard: http://localhost:6333/dashboard\n",
    "- üîç Elasticsearch: http://localhost:9200\n",
    "- ü§ñ Ollama API: http://localhost:11434\n",
    "- üíª VS Code: http://localhost:8080"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
